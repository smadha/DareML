
Dataset and preprocessing: We will use the MiniBooNE particle identification dataset from the UCI Machine Learning Repository. It has 130065 instances with 50 features each and each instance has to be classified as either ”signal” or ”background”. 
  
Download the dataset and call loaddata() in your code to load and process it. The function loads the data, assigns labels to each instance, shuffles the dataset and randomly divides it into training (80%) and test (20%) sets. It also makes your training and test set labels categorical i.e. instead of a scalar ”0” or ”1”, each label becomes a two-dimensional tuple; the new label is (1,0) if the original label is ”0” and it is (0,1) if the original label is ”1”. The dimension of every feature is din = 50 and the dimension of output labels is dout = 2. Next, normalize the features of both the sets by calling normalize() in your code.  

(d) Linear activations: (5 Points) First we will explore networks with linear activations. Train models of the following architectures: [din, dout], [din, 50, dout], [din, 50, 50, dout], [din, 50, 50, 50, dout] each having linear activations for all hidden layers and softmax activation for the last layer. Use 0.0 regularization parameter, set the number of epochs to 30, batch size to 1000, learning rate to 0.001, decay to 0.0, momentum to 0.0, Nesterov flag to False, and Early Stopping to False. Report the test set accuracies and comment on the pattern of test set accuracies obtained. Next, keeping the other parameters same, train on the following architectures: [din, 50, dout], [din, 500, dout], [din, 500, 300, dout], [din, 800, 500, 300, dout], [din, 800, 800, 500, 300, dout]. Report the observations and explain the pattern of test set accuracies obtained. Also report the time taken to train these new set of architectures.    
(e) Sigmoid activation: (5 Points) Next let us try sigmoid activations. We will only explore the bigger architectures though. Train models of the following architectures: [din, 50, dout], [din, 500, dout], [din, 500, 300, dout], [din, 800, 500, 300, dout], [din, 800, 800, 500, 300, dout]; all hidden layers with sigmoids and output layer with softmax. Keep all other parameters the same as with linear activations. Report your test set accuracies and comment on the trend of accuracies obtained with changing model architectures. Also explain why this trend is different from that of linear activations. Report and compare the time taken to train these architectures with those for linear architectures.    
(f) ReLu activation: (5 Points) Repeat the above part with ReLu activations for the hidden layers (output layer = softmax). Keep all other parameters and architectures the same, except change the learning rate to 5 × 10−4. Report your observations and explain the trend again. Also explain why this trend is different from that of linear activations. Report and compare the time taken to train these architectures with those for linear and sigmoid architectures.  
(g) L2-Regularization: (5 Points) Next we will try to apply regularization to our network. For this part we will use a deep network with four layers: [din, 800, 500, 300, dout]; all hidden activations ReLu and output activation softmax. Keeping all other parameters same as for the previous part, train this network for the following set of L2-regularization parameters: [10−7, 5 × 10−7, 10−6, 5 × 10−6, 10−5]. Report your accuracies on the test set and explain the trend of observations. Report the best value of the regularization hyperparameter.  
(h) Early Stopping and L2-regularization: (5 Points) To prevent overfitting, we will next apply early stopping techniques. For early stopping, we reserve a portion of our data as a validation set and if the error starts increasing on it, we stop our training earlier than the provided number of iterations. We will use 10% of our training data as a validation set and stop if the error on the validation set goes up consecutively six times. Train the same architecture as the last part, with the same set of L2-regularization coefficients, but this time set the Early Stopping flag in the call to testmodels() as True. Again report your accuracies on the test set and explain the trend of observations. Report the best value of the regularization hyperparameter this time. Is it the same as with only L2-regularization? Did early stopping help?  
(i) SGD with weight decay: (5 Points) During gradient descent, it is often a good idea to start with a big value of the learning rate (α) and then reduce it as the number of iterations progress.  
￼In this part we will experiment with the decay factor β. Use the network [din, 800, 500, 300, dout]; all hidden activations ReLu and output activation softmax. Use a regularization coefficient = 5 × 10−7, number of epochs = 100, batch size = 1000, learning rate = 10−5, and a list of decays: [10−5, 5 × 10−5, 10−4, 3 × 10−4, 7 × 10−4, 10−3]. Use no momentum and no early stopping. Report your test set accuracies for the decay parameters and choose the best one based on your observations.  
(j) Momentum: (5 Points) Read about momentum for Stochastic Gradient Descent. We will use a variant of basic momentum techniques called the Nesterov momentum. Train the same architecture as in the previous part (with ReLu hidden activations and softmax final activation) with the following parameters: regularization coefficient = 0.0, number of epochs = 50, batch size = 1000, learning rate = 10−5, decay = best value found in last part, Nesterov = True, Early Stopping = False and a list of momentum coefficients = [0.99, 0.98, 0.95, 0.9, 0.85]. Find the best value for the momentum coefficients, which gives the maximum test set accuracy.   
(k) Combining the above: (10 Points) Now train the above architecture: [din, 800, 500, 300, dout] (hidden activations: ReLu and output activation softmax) again, but this time we will use the optimal values of the parameters found in the previous parts. Concretely, use number of epochs = 100, batch size = 1000, learning rate = 10−5, Nesterov = True and Early Stopping = True. For regularization coefficient, decay and momentum coefficient use the best values that you found in the last few parts. Report your test set accuracy again. Is it better or worse than the accuracies you observed in the last few parts?   
(l) Grid search with cross-validation: (15 Points) This time we will do a full fledged search for the best architecture and parameter combinations. Train networks with architectures [din, 50, dout], [din, 500, dout], [din, 500, 300, dout], [din, 800, 500, 300, dout], [din, 800, 800, 500, 300, dout]; hidden activations ReLu and final activation softmax. For each network use the following parameter values: number of epochs = 100, batch size = 1000, learning rate = 10−5, Nesterov = True, Early Stopping = True, Momentum coefficient = 0.99 (this is mostly independent of other values, so we can directly use it without including it in the hyperparameter search). For the other parameters search the full lists: for regularization coefficients = [10−7, 5 × 10−7, 10−6, 5 × 10−6, 10−5], and for decays = [10−5, 5 × 10−5, 10−4]. Report the best parameter values, architecture and the best test set accuracy obtained.  
